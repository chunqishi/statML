%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass{beamer}

\mode<presentation> {

% The Beamer class comes with a number of default slide themes
% which change the colors and layouts of slides. Below this is a list
% of all the themes, uncomment each in turn to see what they look like.

%\usetheme{default}
\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
%\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}

% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

%\usecolortheme{albatross}
%\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
%\usepackage[colorlinks, linkcolor=blue, CJKbookmarks=true]{hyperref}
\usepackage{enumerate}
\usepackage{CJK}
\usepackage{amsmath}




%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title[Language Model]{\href{http://www.statmt.org/book/}{StatML} (Chapter 7): Language Models} % The short title appears at the bottom of every slide, the full title is only on the title page

\author{Chunqi Shi} % Your name
\institute[Brandeis Univ.] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{
Brandeis University \\ % Your institution for the title page
\medskip
%\textit{chunqi.shi@hotmail.com} % Your email address
\textit{shicq@brandeis.edu} % Your email address
}
\date{\today} % Date, can be changed to a custom date

\begin{document}

\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}

\begin{frame}
\frametitle{Overview} % Table of contents slide, comment this block out to remove it
\tableofcontents % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your presentation
\end{frame}

%----------------------------------------------------------------------------------------
%	PRESENTATION SLIDES
%----------------------------------------------------------------------------------------

\section{Language Models}



\begin{frame}\frametitle{Language Models}


\begin{block}{Why?}
Language models answer the question: \\
\quad {\em How \alert{likely} is a string of English words good English?}
\end{block}

\begin{block}{What?}
A statistical language model assigns a \alert{probability} to a sequence of m
words $P(w_1,\ldots,w_m)$ by means of a probability distribution.
\end{block}

\begin{block}{How?}

\begin{itemize}
\item Reordering: \\ \quad  $P_{LM}(\text{the house is small}) > P_{LM}(\text{small the is house})$
\item Word Choice: \\ \quad $P_{LM}(\text{I am going home}) > P_{LM}(\text{I am going house})$
\end{itemize}

\end{block}

\end{frame}



%------------------------------------------------

\begin{frame} \frametitle{Language Models \& SMT Architecture}
How language models work in a basic SMT architecture\footnote{http://slideplayer.us/slide/203403/}?
\begin{figure}
\includegraphics[width=0.6\linewidth]{figure/statml_architecture.PNG}
\label{fig:statml_architecture}
%\caption{}
\end{figure}
\end{frame}

%------------------------------------------------

\begin{frame}[label=go_architecture] \frametitle{Open Source Language Models Example}
Architecture of the LIMSI SMT
system\footnote{http://www.limsi.fr/tlp/mt/} and open language models:
\begin{itemize}
\item SRILM\footnote{http://sourceforge.net/projects/irstlm/} (N-Gram)
  \& NN[Neural Networks] (Continuous Space LM).
\item Giza++: Translation Model. 
\item Moses: Decoder
\end{itemize}
\begin{figure}
\includegraphics[width=0.6\linewidth]{figure/TLP_SMT_arch.png}
\label{fig:TLP_SMT_arch}
%\caption{}
\end{figure}
\hyperlink{go_nnlm}{\beamerbutton{NNLM}}
\end{frame}

%------------------------------------------------
\begin{frame}\frametitle{Other Language Models Applications}

\begin{block}{Speech Recognition}
  \quad $P_{LM}(\text{I saw a van}) > P_{LM}(\text{eyes awe of an})$
\end{block}

\begin{block}{Spell Correction}
The office is about fifteen minuets from my house. \\
  \quad $P_{LM}(\text{about fifteen minutes from}) >
  P_{LM}(\text{about fifteen minuets from})$
\end{block}

\begin{block}{Information Retrieval}
No results found for ``University of Brandeis'' (Query likelihood model).
  \quad $P_{LM}(\text{University of Brandeis}) >
  P_{LM}(\text{Brandeis University})$
\end{block}

\begin{block}{More !!}
Part-of-speech Tagging, Parsing, Summarization, Question-Answering,
etc.
\end{block}
\end{frame}

%------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{N-Gram Language Models}

%------------------------------------------------
\begin{frame}\frametitle{Probabilistic Language Modeling}

\begin{block}{How to Compute $P(W)$}
  \quad $P(W) = P(w_1,\ldots,w_m)$
\end{block}

\begin{block}{Probability of an upcoming word}
 \quad  $P(w_k|w_1, w_2, \ldots, w_{k-1})$
\end{block}

\begin{block}{Decomposing using Chain Rule}
  \quad $P(w_1,\ldots,w_m) = P(w_1)P(w_2|w_1)P(w_2|w_1, w_2)\ldots P(w_m|w_1, w_2, \ldots, w_{m-1})$
\end{block}

\begin{block}{Example}
\quad $P(\text{its water is so transparent}) = $ \\ $P(\text{its})
\times P(\text{water}|\text{its}) \times P(\text{is}|\text{its water})
\times P(\text{so}|\text{its water is}) \times $ \\
$P(\text{transparent}|\text{its water is so})$
\end{block}
\end{frame}

%------------------------------------------------


\begin{frame}\frametitle{Chain Rule Estimation}

\begin{block}{Joint Probability}
  \quad $P(w_1w_2\ldots w_m) = \prod P(w_i|w_1w_2\ldots w_i-1)$
\end{block}

\begin{block}{How to estimate?}
Maximum likelihood estimation: \\
 \quad  $P(\text{transparent}|\text{its water is so}) = $\\ 
$$\frac{Count(\text{its water is so transparent})}{Count(\text{its water is so})}$$
\end{block}

\begin{block}{Problems?}
\begin{itemize}
\item Sparse data: NO enough data for estimating.
\item Large space: HUGE possible sentences.
\end{itemize}
\end{block}

\end{frame}


%------------------------------------------------
\begin{frame}\frametitle{Markov Chain}

\begin{block}{Markov Assumption}
  Only previous history matters: \\
 \quad  $P(\text{transparent}|\text{its water is so}) =
 (\text{transparent}|\text{so})$  or maybe \\
 \quad  $P(\text{transparent}|\text{its water is so}) = 
 (\text{transparent}|\text{so})$  
\end{block}

\begin{block}{$k_{\text{th}}$ Order Markov Model}
 \quad $P(w_1w_2\ldots w_m) = \prod P(w_i|w_{i-k}w_2\ldots w_i-1)$
\end{block}

\begin{block}{Simple Cases}
Unigram model:
 \quad $P(w_1w_2\ldots w_m) = \prod P(w_i)$

Bigram model:
 \quad $P(w_1w_2\ldots w_m) = \prod P(w_i|w_{i-1})$
\end{block}

\end{frame}


%------------------------------------------------

%------------------------------------------------
\begin{frame}\frametitle{N-gram Models}

\begin{block}{Is Markov assumption sufficient? \alert{NO!}}
Language has long-distance dependencies:

\begin{figure}
\includegraphics[width=0.6\linewidth]{figure/long-distance-dependency.png}
\label{fig:long-distance-dependency}
\end{figure}

Or:  \\

``The computer which I had just put into the machine room on the fifth
floor crashed.'' 

\end{block}
\end{frame}


%------------------------------------------------
%------------------------------------------------
\begin{frame}\frametitle{Bigram Example}
\begin{figure}
\includegraphics[width=1\linewidth]{figure/bigram_example.pdf}
\label{fig:bigram_example}
\end{figure}

\end{frame}


%------------------------------------------------
%------------------------------------------------
\begin{frame}\frametitle{Trigram Example}
\begin{figure}
\includegraphics[width=1\linewidth]{figure/trigram_example.pdf}
\label{fig:trigram_example}
\end{figure}

``The red cross'' and ``The green party'' are frequent trigrams in the
Europarl corpus.
\end{frame}


%------------------------------------------------

%------------------------------------------------
\begin{frame}\frametitle{Evaluation of N-gram Models}

\begin{block}{How good is our model?}
Extrinsic Evaluation: training A \& B, testing, comparing accuracy of
A \& B by evaluation metric. \\

But it is \alert{time-consuming}.
\end{block}

\begin{block}{Intrinsic Evaluation}
Perplexity: How well can we predict the next word?

Intrinsic evaluation is \alert{Bad approximation}!
Unless the test data looks just like the training data. 

But is helpful to think about. 
\end{block}

\begin{block}{Intuition of Perplexity}
How hard is the task of recognizing digits ``0, 1, 2, 3, 4, 5, 6, 7,
8, 9''?

Perplexity 10.


\end{block}
\end{frame}


%------------------------------------------------
%------------------------------------------------
\begin{frame}\frametitle{Perplexity}

\begin{block}{Cross Entropy}
\begin{align*}
& H(W) =  -\frac{1}{n}\log{P(w_1w_2\ldots w_n)}  \\
& = -\frac{1}{n}\sum_i^n\log{P(w_i|w_1\ldots,w_{i-1})} 
\end{align*}
Perplexity:
$$
PP(W) = 2^{H(W)} = P(W)^{-\frac{1}{n}}
$$
\end{block}

\begin{block}{Perplexity as branching factor}
\begin{align*}
& PP(W) =  P(1\text{, }2\text{, }\ldots\text{, } 10)^{-\frac{1}{10}}  \\
& =(\frac{1}{10})^{10\times -\frac{1}{10}} = (\frac{1}{10})^{-1} = 10 
\end{align*}
\end{block}

\end{frame}


%------------------------------------------------
%------------------------------------------------
\begin{frame}\frametitle{Comparison N-gram Models}

Minimizing perplexity is the same as maximizing probability, thus
better model.

\begin{figure}
\includegraphics[width=0.6\linewidth]{figure/comparison_perplexity.pdf}
\label{fig:comparison_perplexity}
\end{figure}

\end{frame}


%------------------------------------------------
%------------------------------------------------
\begin{frame}\frametitle{Generalization and Zeros}

\begin{block}{Unseen N-grams}
Things that NOT ever occur in the training set. But occur in the test
set. 

\end{block}

\begin{columns}[t] % The "c" option specifies centered vertical alignment while the "t" option is used for top vertical alignment
\column{.45\textwidth} % Left column and width
Training Set:
\begin{enumerate}
\item ... denied the allegations
\item ... denied the reports
\item ... denied the claims
\item ... denied the request
\end{enumerate}

\column{.5\textwidth} % Right column and width
Test Set:
\begin{enumerate}
\item ... denied the offer
\item ... denied the loan
\end{enumerate}
\end{columns}
$$
P(\text{``offer''}|\text{''denied the''}) = 0
$$

\begin{block}{Smoothing}
Sparse statistics, smoothing to generalize better.
\end{block}
\end{frame}


%------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Smoothing}

%------------------------------------------------
\begin{frame}\frametitle{Smoothing}

\begin{block}{How to smooth all words non-zeros}

\begin{figure}
\includegraphics[width=0.9\linewidth]{figure/smoothing.pdf}
\label{fig:smoothing}
\end{figure}
\end{block}
\end{frame}


%------------------------------------------------

%------------------------------------------------
\begin{frame}\frametitle{Smoothing Methods}

\begin{enumerate}
\item Additive Smoothing:  Laplace (18th century) came up with this smoothing
  technique when he tried to estimate the chance that the sun will
  rise tomorrow (sunrise problem).  
\item Good-Turing Estimate: Alan Turing and his assistant I. J. Good
  (1953) as part of their efforts to crack German ciphers for the Enigma
 machine during World War II. 
% NLP professor Geoffrey Sampson (1995) implemented a simplified and
% easier-to-use variant. 

\item Jelinek-Mercer Smoothing (linear interpolation):  Previous IBM workers
  (1980) Frederick Jelinek and Robert Mercer received ACL Lifetime
  Achievement Award (The Dawn of Statistical ASR and MT) in June 2014.  

\end{enumerate}

\end{frame}


%------------------------------------------------
%------------------------------------------------
\begin{frame}\frametitle{Smoothing Methods}

\begin{enumerate}
\item Katz Smoothing (back-off):  Slava M. Katz (1987) proposed
  non-linear estimation of probabilities from sparse data for the
  language model component of a speech recogniser.  
\item Witten-Bell Smoothing: Ian H. Witten and Timothy C. Bell (1991)
  proposed estimating the probabilities of novel events in adaptive
  text compression.  
\item Kneser-Ney Smoothing:
   Kneser and Ney (1995) proposed ``improved backing-off for m-gram language
   modeling'',  evolved from absolute-discounting interpolation, which
   is one of the best smoothing methods for n-gram language.  

\end{enumerate}

\end{frame}

%------------------------------------------------
%------------------------------------------------
\begin{frame}\frametitle{Add-One Smoothing}

\begin{block}{Laplace smoothing}
Pretend we saw each word one more time than we did.

\begin{figure}
\includegraphics[width=0.6\linewidth]{figure/laplace_smoothing.pdf}
\label{fig:laplace_smoothing}
\end{figure}

\end{block}
\end{frame}


%------------------------------------------------
%------------------------------------------------
\begin{frame}\frametitle{Bigram Add-One Smoothing}
\begin{figure}
\includegraphics[width=0.9\linewidth]{figure/bigram_addone_smoothing.pdf}
\label{fig:bigram_addone_smoothing}
\end{figure}

\end{frame}


%------------------------------------------------
%------------------------------------------------
\begin{frame}\frametitle{Add-$\alpha$ Smoothing}

\begin{block}{Will $\alpha$ adjusted count a lot?}
\begin{figure}
\includegraphics[width=0.8\linewidth]{figure/add_alpha_smoothing.pdf}
\label{fig:add_alpha_smoothing}
\end{figure}
\end{block}

\end{frame}


%------------------------------------------------
%------------------------------------------------
\begin{frame}\frametitle{Comparison of Add-$\alpha$ Smoothing}

\begin{block}{Bigram in Europarl corpus}

\begin{figure}
\includegraphics[width=0.9\linewidth]{figure/bigram_alpha_smoothing.pdf}
\label{fig:bigram_alpha_smoothing}
\end{figure}

\end{block}
\end{frame}


%------------------------------------------------
% %------------------------------------------------
% \begin{frame}\frametitle{N-gram Models}

% \begin{block}{Is Markov assumption sufficient? \alert{NO!}}
% Language has long-distance dependencies:

% \begin{figure}
% \includegraphics[width=0.6\linewidth]{figure/add_alpha_smoothing.pdf}
% \label{fig:add_alpha_smoothing}
% \end{figure}
% \end{block}

% \end{frame}


% %------------------------------------------------
% %------------------------------------------------
% \begin{frame}\frametitle{Comparison of Add-$\alpha$ Smoothing}

% \begin{block}{Bigram in Europarl corpus}

% \begin{figure}
% \includegraphics[width=0.9\linewidth]{figure/bigram_alpha_smoothing.pdf}
% \label{fig:bigram_alpha_smoothing}
% \end{figure}

% \end{block}
% \end{frame}


% %------------------------------------------------
%------------------------------------------------
\begin{frame}\frametitle{Held-out Estimation}

\begin{block}{Long Tail of N-gram Counting}
\begin{itemize}
\item 1,266,566 bigrams in this corpus, more than
half, 753,777, occur only once.
\item Zipf's Law:  the frequency of any word is inversely proportional to its rank in the frequency table. 
\end{itemize}

If we observe an n-gram
c times in the training corpus, ow often
do we expect it to see in the future (Held-out Estimation)?

$p_h(w_1w_2\ldots w_n) = \frac{E[r]}{N_h} \sim \frac{T_r}{N_r\times
  N_h}$,
$r = count_h(w_1w_2\ldots w_n)$
\end{block}

\begin{block}{Cross-Validation Estimation}
$$E(r) = \frac{T_r^1+T_r^2}{N_r^1+N_r^2}$$
\end{block}


\end{frame}


%------------------------------------------------
%------------------------------------------------
\begin{frame}\frametitle{Deleted Estimation}

Deleted Estimation: leave one part of the training corpus out
for validation.

\begin{columns}[c] % The "c" option specifies centered vertical alignment while the "t" option is used for top vertical alignment
\column{.5\textwidth} % Left column and width

\begin{figure}
\includegraphics[width=0.9\linewidth]{figure/deleted_estimation.pdf}
\label{fig:deleted_estimation}
\end{figure}

\column{.5\textwidth} % Right column and width

\begin{figure}
\includegraphics[width=0.9\linewidth]{figure/test_count.pdf}
\label{fig:test_acount}
\end{figure}

\end{columns}

\end{frame}


%------------------------------------------------


%------------------------------------------------
\begin{frame}\frametitle{Good-Turing Smoothing Intuition}
\begin{figure}
\includegraphics[width=1\linewidth]{figure/good_turing_smoothing.pdf}
\label{fig:good_turing_smoothing}
\end{figure}
\end{frame}


%------------------------------------------------
%------------------------------------------------
\begin{frame}\frametitle{Good-Turing Smoothing Calculation}
\begin{figure}
\includegraphics[width=0.9\linewidth]{figure/good_turing_calculation.pdf}
\label{fig:good_turing_calculation}
\end{figure}
\end{frame}


%------------------------------------------------
%------------------------------------------------
\begin{frame}\frametitle{Leave One Out Intuition}
\begin{figure}
\includegraphics[width=0.9\linewidth]{figure/good_turing_loo_intuition.pdf}
\label{fig:good_turing_loo_intuition}
\end{figure}
\end{frame}


%------------------------------------------------
%------------------------------------------------
\begin{frame}[label=go_good_turing]\frametitle{Good-Turing for 2-Grams in Europarl}
\begin{figure}
\includegraphics[width=0.9\linewidth]{figure/good_turing_2gram.pdf}
\end{figure}
\hyperlink{go_absolute_discounting}{\beamergotobutton{approximation}}.
\end{frame}


%------------------------------------------------
%------------------------------------------------
\begin{frame}\frametitle{Derivation of Good-Turing}
\begin{figure}
\includegraphics[width=0.9\linewidth]{figure/derivation_good_turing.pdf}
\end{figure}
\end{frame}


%------------------------------------------------
%------------------------------------------------
\begin{frame}\frametitle{Derivation of Good-Turing (2)}
\begin{figure}
\includegraphics[width=0.9\linewidth]{figure/derivation_good_turing_2.pdf}
\end{figure}
\end{frame}


%------------------------------------------------
%------------------------------------------------
\begin{frame}\frametitle{Derivation of Good-Turing (3)}
\begin{figure}
\includegraphics[width=0.9\linewidth]{figure/derivation_good_turing_3.pdf}
\end{figure}
\end{frame}


%------------------------------------------------
%------------------------------------------------
\begin{frame}\frametitle{Derivation of Good-Turing (4)}
\begin{figure}
\includegraphics[width=0.9\linewidth]{figure/derivation_good_turing_4.pdf}
\end{figure}
\end{frame}


%------------------------------------------------
%------------------------------------------------
\begin{frame}\frametitle{Derivation of Good-Turing (5)}
\begin{figure}
\includegraphics[width=0.9\linewidth]{figure/derivation_good_turing_5.pdf}
\end{figure}
\end{frame}


%------------------------------------------------
%------------------------------------------------
\begin{frame}\frametitle{Derivation of Good-Turing (6)}
\begin{figure}
\includegraphics[width=0.9\linewidth]{figure/derivation_good_turing_6.pdf}
\end{figure}
\end{frame}


%------------------------------------------------
%------------------------------------------------
\begin{frame}\frametitle{Derivation of Good-Turing (7)}
\begin{figure}
\includegraphics[width=0.9\linewidth]{figure/derivation_good_turing_7.pdf}
\end{figure}
\end{frame}


%------------------------------------------------
%------------------------------------------------
\begin{frame}\frametitle{Derivation of Good-Turing (8)}
\begin{figure}
\includegraphics[width=0.9\linewidth]{figure/derivation_good_turing_8.pdf}
\end{figure}
\end{frame}


%------------------------------------------------
%------------------------------------------------
\begin{frame}\frametitle{Derivation of Good-Turing (9)}
\begin{figure}
\includegraphics[width=0.8\linewidth]{figure/derivation_good_turing_9.pdf}
\end{figure}

\begin{block}{Intuition}
Good-Turing allows us to estimate the probability mass assigned to n-grams with lower counts by
looking at the number of n-grams with higher counts.  
\end{block}

\end{frame}


%------------------------------------------------



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Interpolation \& Back-off}



%------------------------------------------------
\begin{frame}\frametitle{Interpolation \& Back-off}

\begin{block}{Sparseness in a Trigram Model}

\begin{enumerate}
\item Back to bigrams, otherwise unigram: \alert{Back-off}
\item Mix that model with bigram and unigram: \alert{Interpolation} 
\end{enumerate}

\end{block}


\begin{block}{Context}
\begin{enumerate}
\item \alert{Back-off}: 
some times it helps to use less
context condition on less context for contexts
you haven't learned much about.

\item \alert{Interpolation}: 
higher and lower order n-gram models have different strengths and weaknesses:
\begin{itemize}
\item high-order n-grams are sensitive to more context, but have sparse counts.
\item low-order n-grams consider only very limited context, but have robust counts
\end{itemize}
\end{enumerate}

\end{block}


\end{frame}


%------------------------------------------------

%------------------------------------------------
\begin{frame}\frametitle{Interpola1on (Jelinek-Mercer Smoothing)}

\begin{block}{Simple interpolation}
\vspace{-15pt}
\begin{align*}
\hat{P}(w_n|w_{n-1}w_{n-2})= & \lambda_1 P(w_n|w_{n-1}w_{n-2}) + \lambda_2 P(w_n|w_{n-1}) + \\
& (1-\lambda_1 - \lambda_2) P(w_n) 
\end{align*}
\end{block}

\begin{block}{Recursive Interpolation}
Lambdas conditional on context:
\vspace{-5pt}
\begin{align*}
\hat{P}(w_n|w_{n-1}w_{n-2})= & \lambda_1(w_{n-2}^{n-1}) P(w_n|w_{n-1}w_{n-2}) + \lambda_2(w_{n-2}^{n-1}) P(w_n|w_{n-1}) + \\
& (1-\lambda_1(w_{n-2}^{n-1}) - \lambda_2(w_{n-2}^{n-1})) P(w_n) 
\end{align*}
\vspace{-10pt}
{\footnotesize
\begin{align*}
\sum_{w\in V}\hat{P}(w_n|w_{n-1}w_{n-2})= &
\lambda_1(w_{n-2}^{n-1}) \sum_{w\in V}  P(w_n|w_{n-1}w_{n-2}) + \lambda_2(w_{n-2}^{n-1})\sum_{w\in V}  P(w_n|w_{n-1}) + \\
& (1-\lambda_1(w_{n-2}^{n-1}) -
\lambda_2(w_{n-2}^{n-1})) \sum_{w\in V} P(w_n)\\
& =  \lambda_1(w_{n-2}^{n-1}) + \lambda_2(w_{n-2}^{n-1}) + (1-\lambda_1(w_{n-2}^{n-1}) -
\lambda_2(w_{n-2}^{n-1})) = 1
\end{align*}
}%
\end{block}


\end{frame}


%------------------------------------------------

%------------------------------------------------
\begin{frame}\frametitle{How to Set the Lambdas?}
\begin{figure}
\includegraphics[width=0.9\linewidth]{figure/lambda_set_interpolation.pdf}
\end{figure}

\begin{block}{Intuition}
Jelinek and Mercer smoothing uses the lower order n-grams in combination with maximum
likelihood estimation.
\end{block}

\end{frame}
%------------------------------------------------

%------------------------------------------------
\begin{frame}\frametitle{Back-Off}

\begin{block}{Prediction Model}
Models are defined recursively in terms of lower order models.
\begin{figure}
\includegraphics[width=0.7\linewidth]{figure/back_off.pdf}
\end{figure}

Discounting function: $d_n(w_{i-n+1}, \ldots, w_{i-1})$

Back-off Weight: $\alpha_{w_{i-n+1} \cdots w_{i -1}}$
\end{block}



\end{frame}


%------------------------------------------------
%------------------------------------------------
\begin{frame}\frametitle{Katz Smoothing (Back-Off with Good-Turing)}
Katz Adjusted Probability:
\begin{block}{Discounting function}
If Good-Turing smoothing adjusts C into $C^*$ and
$ P (w_i | w_{i-n+1} \cdots w_{i-1}) =
\frac{C(w_{i-n+1}...w_{i-1}w_{i})}{C(w_{i-n+1} \cdots w_{i-1})}$, 
then $$d = \frac{C^*}{C}$$
\end{block}


\begin{block}{Back-off Weight}
\vspace{-8pt}
$$
\beta_{w_{i-n+1} \cdots w_{i -1}} = 1 - \sum_{ \{w_i : C(w_{i-n+1}
  \cdots w_{i}) > k \} } d_{w_{i-n+1} \cdots w_{i}}
\frac{C(w_{i-n+1}...w_{i-1} w_{i})}{C(w_{i-n+1} \cdots w_{i-1})} 
$$

$$
\alpha_{w_{i-n+1} \cdots w_{i -1}} = \frac{\beta_{w_{i-n+1} \cdots
    w_{i -1}}}        {\sum_{ \{ w_i : C(w_{i-n+1} \cdots w_{i}) \leq
    k \} } P_{bo}(w_i | w_{i-n+2} \cdots w_{i-1})}
$$
\end{block}

\end{frame}


%------------------------------------------------

%------------------------------------------------
\begin{frame}\frametitle{Katz Adjusted Counts}
\begin{figure}
\includegraphics[width=0.9\linewidth]{figure/katz_bigram.pdf}
\end{figure}
\end{frame}
%------------------------------------------------
%------------------------------------------------
\begin{frame}\frametitle{Bigrams Katz Smoothing}
\begin{figure}
\includegraphics[width=0.9\linewidth]{figure/katz_bigram_discounting.pdf}
\end{figure}
\end{frame}
%------------------------------------------------
%------------------------------------------------
\begin{frame}\frametitle{Diversity of Predicted Words}
\begin{figure}
\includegraphics[width=1\linewidth]{figure/predicted_words.pdf}
\end{figure}
\begin{block}{Witten-Bell}
Think of unseen events as ones not having
happened yet; The probability of this event, when it happens, can
be modeled by the probability of seeing it for the
first time.
\end{block}
\end{frame}
%------------------------------------------------
%------------------------------------------------
\begin{frame}\frametitle{First Time Words}
\begin{figure}
\includegraphics[width=1\linewidth]{figure/witten_Bell_words.pdf}
\end{figure}
\end{frame}
%------------------------------------------------
%------------------------------------------------
\begin{frame}\frametitle{First Time Estimation}
\begin{figure}
\includegraphics[width=1\linewidth]{figure/witten_Bell_smooth.pdf}
\end{figure}
\end{frame}
%------------------------------------------------

%------------------------------------------------
\begin{frame}\frametitle{Witten-Bell Smoothing}

\begin{block}{An Instance of Jelinek-Mercer Smoothing}
Recursive interpolation method of using higher-order model.
$$
P^{WB}(w_i|w_{i-n+1}^{i-1}) =
\lambda_{w_{i-n+1}^{i-1}}P(w_i|w_{i-n+1}^{i-1}) + (1-\lambda_{w_{i-n+1}^{i-1}})P^{WB}(w_i|w_{i-n+2}^{i-1})
$$
\vspace{-10pt}
\begin{figure}
\includegraphics[width=1\linewidth]{figure/witten_bell_smoothing_equation.pdf}
\end{figure}
\end{block}
\end{frame}


%------------------------------------------------
%------------------------------------------------
\begin{frame}\frametitle{Witten-Bell Smoothing Example}

\begin{block}{}
\begin{figure}
\includegraphics[width=1\linewidth]{figure/witten_bell_exam.pdf}
\end{figure}
\end{block}
\end{frame}


%------------------------------------------------




%------------------------------------------------
\begin{frame}\frametitle{Comparing to Good-Turing Smoothing}
\begin{figure}
\includegraphics[width=0.9\linewidth]{figure/good_turing_witten_bell.pdf}
\end{figure}
\end{frame}
%------------------------------------------------

% %------------------------------------------------
% \begin{frame}\frametitle{Back-Off with Good-Turing Smoothing}
% \begin{figure}
% \includegraphics[width=0.9\linewidth]{figure/back_off_good_turing.pdf}
% \end{figure}
% \end{frame}


% %------------------------------------------------

%------------------------------------------------

%------------------------------------------------

\begin{frame}\frametitle{Diversity of History}
\begin{figure}
\includegraphics[width=0.9\linewidth]{figure/diversity_history.pdf}
\end{figure}
\end{frame}
%------------------------------------------------

%------------------------------------------------

\begin{frame}\frametitle{Kneser-Ney Smoothing}
\begin{figure}
\includegraphics[width=0.9\linewidth]{figure/kneser_ney_continuation.pdf}
\end{figure}
\end{frame}
%------------------------------------------------



%------------------------------------------------
\begin{frame}[label=go_absolute_discounting]\frametitle{Absolute Discounting Interpolation}
Good Turing Smoothing is used in Katz smoothing. If we look into 
it, might save some time by calculating. 
\hyperlink{go_good_turing}{\beamergotobutton{Good Turing}}

%\begin{block}{}
Chen and Goodman suggested absolute discounting. 
\begin{figure}
\includegraphics[width=1\linewidth]{figure/absolute_discounting.pdf}
\end{figure}
%\end{block}
\end{frame}


%------------------------------------------------
%------------------------------------------------

\begin{frame}\frametitle{Parameters of Absolute Discounting}
\begin{figure}
\includegraphics[width=0.9\linewidth]{figure/absolute_discounting_para.pdf}
\end{figure}
\end{frame}
%------------------------------------------------



%------------------------------------------------
\begin{frame}\frametitle{Kneser-Ney Smoothing}
%\begin{block}{}
\vspace{-3pt}
\begin{figure}
\includegraphics[width=1\linewidth]{figure/modified_kneser_ney.pdf}
\end{figure}
\vspace{-7pt}
\begin{figure}
\includegraphics[width=1\linewidth]{figure/modified_kneser_ney_low_order.pdf}
\end{figure}

%\end{block}
\end{frame}


%------------------------------------------------
%------------------------------------------------
\begin{frame}\frametitle{Lower Order Formula for $\gamma(d)$}
%\begin{block}{}
\begin{figure}
\includegraphics[width=1\linewidth]{figure/modified_kneser_ney_high_order.pdf}
\end{figure}
%\end{block}
\end{frame}
%------------------------------------------------


%------------------------------------------------
\begin{frame}\frametitle{Interpolated Back-off}
%\begin{block}{}
\begin{figure}
\includegraphics[width=1\linewidth]{figure/interpolated_back_off.pdf}
\end{figure}
%\end{block}
\end{frame}
%------------------------------------------------

%------------------------------------------------
\begin{frame}\frametitle{Interpolation \& Back-off}

\begin{itemize}
\item Both interpolation (Jelinek-Mercer) and backoff (Katz) involve combining
information from \alert{higher- and lower-order} models.
\item Key difference: in determining the probability of n-grams with \alert{nonzero}
counts, interpolated models use information from lower-order models
while backoff models do not.
\item In both backoff and interpolated models, lower-order models are
used in determining the probability of n-grams with \alert{zero counts
(sparsity)}.
\item It turns out that it's \alert{not hard} to create a backoff version of an
interpolated algorithm, and vice-versa. (Kneser-Ney was originally
backoff; Chen \& Goodman made interpolated version.)
\end{itemize}
\end{frame}
%------------------------------------------------


%------------------------------------------------
\begin{frame}\frametitle{Summary of Smoothing}
By Chen \& Goodman (1998)
\begin{itemize}
\item  The factor with the largest influence is the use of a modified backoff
distribution as in \alert{Kneser-Ney smoothing}.
\item  Jelinek-Mercer performs better on small training sets; Katz performs
better on large training sets.
\item  Katz smoothing performs well on n-grams with large counts; Kneser-
Ney is best for small counts.
\item  \alert{Absolute discounting} is superior to linear discounting.
\item  Interpolated models are superior to backoff models for low (nonzero)
counts.
\item  Adding free parameters to an algorithm and optimizing these parameters
on \alert{held-out} data can improve performance.
\end{itemize}
\end{frame}
%------------------------------------------------

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Size of Language Models}

%------------------------------------------------
\begin{frame}\frametitle{Evaluation}
%\begin{block}{}
\begin{figure}
\includegraphics[width=1\linewidth]{figure/evaluation.pdf}
\end{figure}
%\end{block}
\end{frame}
%------------------------------------------------


%------------------------------------------------
\begin{frame}\frametitle{Managing the Size of the Model}
%\begin{block}{}
\begin{figure}
\includegraphics[width=0.8\linewidth]{figure/size_of_model.pdf}
\end{figure}
%\end{block}
\end{frame}
%------------------------------------------------


%------------------------------------------------
\begin{frame}\frametitle{Number of Unique N-Grams}
%\begin{block}{}
\begin{figure}
\includegraphics[width=0.8\linewidth]{figure/number_of_ngram.pdf}
\end{figure}
%\end{block}
\end{frame}
%------------------------------------------------


%------------------------------------------------
\begin{frame}\frametitle{Estimation on Disk}
%\begin{block}{}
\begin{figure}
\includegraphics[width=0.9\linewidth]{figure/estimiation_on_disk.pdf}
\end{figure}
%\end{block}
\end{frame}
%------------------------------------------------


%------------------------------------------------
\begin{frame}\frametitle{Efficient Data Structures}
%\begin{block}{}
\begin{figure}
\includegraphics[width=1\linewidth]{figure/efficient_data_structure.pdf}
\end{figure}
%\end{block}
\end{frame}
%------------------------------------------------

%------------------------------------------------
\begin{frame}\frametitle{Fewer Bits to Store Probabilities}
%\begin{block}{}
\begin{figure}
\includegraphics[width=1\linewidth]{figure/store_probabilities.pdf}
\end{figure}
%\end{block}
\end{frame}
%------------------------------------------------

%------------------------------------------------
\begin{frame}\frametitle{Reducing Vocabulary Size}
%\begin{block}{}
\begin{figure}
\includegraphics[width=1\linewidth]{figure/vocabulary_size.pdf}
\end{figure}
%\end{block}
\end{frame}
%------------------------------------------------

%------------------------------------------------
\begin{frame}\frametitle{Filtering Irrelevant N-Grams}
%\begin{block}{}
\begin{figure}
\includegraphics[width=1\linewidth]{figure/filter_ngram.pdf}
\end{figure}
%\end{block}
\end{frame}
%------------------------------------------------

\section{Concept of Nenural Network Language Model}
%------------------------------------------------
\begin{frame}[label=go_nnlm]\frametitle{Statistical Language Models Based on \textbf{N}eural
    \textbf{N}etworks}
By Tomas Mikolov, Google (2012)
\begin{block}{Motivation}
A good model of language, meaningful sentences should be more likely
than the ambiguous ones.
\end{block}

\begin{block}{Limitations of N-gram Models}
\begin{itemize}
\item Many histories $h$ are similar, but n-grams assume exact
match of $h$.
\item Practically, n-grams have problems with representing
patterns over more than a few words
\item With increasing order of the n-gram model, the number of
possible parameters increases \alert{exponentially}
\item There will be never enough of the training data to estimate
parameters of high-order N-gram models
\end{itemize}
\end{block}
\hyperlink{go_architecture}{\beamerbutton{Architecture}}
\end{frame}
%------------------------------------------------


%------------------------------------------------
\begin{frame}\frametitle{NN Based LM}

\begin{itemize}
\item The sparse history $h$ is projected into some continuous
low-dimensional space, where similar histories get
clustered
\item Thanks to parameter sharing among similar histories, the
model is more robust: less parameters have to be
estimated from the training data
\end{itemize}
\end{frame}
%------------------------------------------------
%------------------------------------------------
\begin{frame}\frametitle{Model Description - Feedforward NNLM}
\begin{figure}
\includegraphics[width=0.8\linewidth]{figure/nnlm.pdf}
\end{figure}
\end{frame}
%------------------------------------------------
%------------------------------------------------
\begin{frame}\frametitle{Model description - recurrent NNLM}
\begin{figure}
\includegraphics[width=0.75\linewidth]{figure/rnnlm.pdf}
\end{figure}
\end{frame}
%------------------------------------------------

%------------------------------------------------
\begin{frame}\frametitle{Model Description - Recurrent NNLM}
\begin{figure}
\includegraphics[width=0.8\linewidth]{figure/rnnlm_description.pdf}
\end{figure}
\end{frame}
%------------------------------------------------

%------------------------------------------------
\begin{frame}\frametitle{Part of Results}
\begin{figure}
\includegraphics[width=0.65\linewidth]{figure/nnlm_rst1.pdf}
\end{figure}
\vspace{-8pt}
\begin{figure}
\includegraphics[width=0.7\linewidth]{figure/nnlm_rst2.pdf}
\end{figure}
\end{frame}
%------------------------------------------------
%------------------------------------------------
\begin{frame}\frametitle{RNNLM Conclusion}
\begin{block}{Beyond N-grams?}
\begin{itemize}
\item RNN LMs can generate much more meaningful text than
n-gram models trained on the same data
\item Many novel but meaningful sequences of words were
generated
\item RNN LMs are clearly better at modeling the language than
n-grams
\item However, many simple patterns in the language cannot be
efficiently described even by RNNs...
\end{itemize}
\end{block}
\end{frame}
%------------------------------------------------




















% %------------------------------------------------
% \section{First Section} % Sections can be created in order to organize your presentation into discrete blocks, all sections and subsections are automatically printed in the table of contents as an overview of the talk
% %------------------------------------------------

% \subsection{Subsection Example} % A subsection can be created just before a set of slides with a common theme to further break down your presentation into chunks

% \begin{frame}
% \frametitle{Paragraphs of Text}
% Sed iaculis dapibus gravida. Morbi sed tortor erat, nec interdum arcu. Sed id lorem lectus. Quisque viverra augue id sem ornare non aliquam nibh tristique. Aenean in ligula nisl. Nulla sed tellus ipsum. Donec vestibulum ligula non lorem vulputate fermentum accumsan neque mollis.\\~\\

% Sed diam enim, sagittis nec condimentum sit amet, ullamcorper sit amet libero. Aliquam vel dui orci, a porta odio. Nullam id suscipit ipsum. Aenean lobortis commodo sem, ut commodo leo gravida vitae. Pellentesque vehicula ante iaculis arcu pretium rutrum eget sit amet purus. Integer ornare nulla quis neque ultrices lobortis. Vestibulum ultrices tincidunt libero, quis commodo erat ullamcorper id.
% \end{frame}

% %------------------------------------------------

% \begin{frame}
% \frametitle{Bullet Points}
% \begin{itemize}
% \item Lorem ipsum dolor sit amet, consectetur adipiscing elit
% \item Aliquam blandit faucibus nisi, sit amet dapibus enim tempus eu
% \item Nulla commodo, erat quis gravida posuere, elit lacus lobortis est, quis porttitor odio mauris at libero
% \item Nam cursus est eget velit posuere pellentesque
% \item Vestibulum faucibus velit a augue condimentum quis convallis nulla gravida
% \end{itemize}
% \end{frame}

% %------------------------------------------------

% \begin{frame}
% \frametitle{Blocks of Highlighted Text}
% \begin{block}{Block 1}
% Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer lectus nisl, ultricies in feugiat rutrum, porttitor sit amet augue. Aliquam ut tortor mauris. Sed volutpat ante purus, quis accumsan dolor.
% \end{block}

% \begin{block}{Block 2}
% Pellentesque sed tellus purus. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Vestibulum quis magna at risus dictum tempor eu vitae velit.
% \end{block}

% \begin{block}{Block 3}
% Suspendisse tincidunt sagittis gravida. Curabitur condimentum, enim sed venenatis rutrum, ipsum neque consectetur orci, sed blandit justo nisi ac lacus.
% \end{block}
% \end{frame}

% %------------------------------------------------

% \begin{frame}
% \frametitle{Multiple Columns}
% \begin{columns}[c] % The "c" option specifies centered vertical alignment while the "t" option is used for top vertical alignment

% \column{.45\textwidth} % Left column and width
% \textbf{Heading}
% \begin{enumerate}
% \item Statement
% \item Explanation
% \item Example
% \end{enumerate}

% \column{.5\textwidth} % Right column and width
% Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer lectus nisl, ultricies in feugiat rutrum, porttitor sit amet augue. Aliquam ut tortor mauris. Sed volutpat ante purus, quis accumsan dolor.

% \end{columns}
% \end{frame}

% %------------------------------------------------
% \section{Second Section}
% %------------------------------------------------

% \begin{frame}
% \frametitle{Table}
% \begin{table}
% \begin{tabular}{l l l}
% \toprule
% \textbf{Treatments} & \textbf{Response 1} & \textbf{Response 2}\\
% \midrule
% Treatment 1 & 0.0003262 & 0.562 \\
% Treatment 2 & 0.0015681 & 0.910 \\
% Treatment 3 & 0.0009271 & 0.296 \\
% \bottomrule
% \end{tabular}
% \caption{Table caption}
% \end{table}
% \end{frame}

% %------------------------------------------------

% \begin{frame}
% \frametitle{Theorem}
% \begin{theorem}[Mass--energy equivalence]
% $E = mc^2$
% \end{theorem}
% \end{frame}

% %------------------------------------------------

% \begin{frame}[fragile] % Need to use the fragile option when verbatim is used in the slide
% \frametitle{Verbatim}
% \begin{example}[Theorem Slide Code]
% \begin{verbatim}
% \begin{frame}
% \frametitle{Theorem}
% \begin{theorem}[Mass--energy equivalence]
% $E = mc^2$
% \end{theorem}
% \end{frame}\end{verbatim}
% \end{example}
% \end{frame}

% %------------------------------------------------

% \begin{frame}
% \frametitle{Figure}
% Uncomment the code on this slide to include your own image from the same directory as the template .TeX file.
% %\begin{figure}
% %\includegraphics[width=0.8\linewidth]{test}
% %\end{figure}
% \end{frame}

% %------------------------------------------------

% \begin{frame}[fragile] % Need to use the fragile option when verbatim is used in the slide
% \frametitle{Citation}
% An example of the \verb|\cite| command to cite within the presentation:\\~

% This statement requires citation \cite{p1}.
% \end{frame}

% %------------------------------------------------

% % http://tex.stackexchange.com/questions/147279/references-at-the-end-of-beamer-slides



% \begin{frame}
% \frametitle{References}
% \footnotesize{
% \begin{thebibliography}{99} % Beamer does not support BibTeX so references must be inserted manually as below
% \bibitem[Smith, 2012]{p1} John Smith (2012)
% \newblock Title of the publication
% \newblock \emph{Journal Name} 12(3), 45 -- 678.
% \end{thebibliography}
% }
% \end{frame}

%------------------------------------------------

%------------------------------------------------

% http://tex.stackexchange.com/questions/147279/references-at-the-end-of-beamer-slides





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{End}

%------------------------------------------------
\begin{frame}\frametitle{Summary}

\begin{itemize}
\item  Language models: How \alert{likely} is a string of English words
  good English? 
\item  N-gram models: Markov assumption (useful but \alert{not} sufficient) 
\item  Perplexity (intrinsic evaluation)
\item  Count smoothing (sparsity \& large space)
\begin{enumerate}
\item  Lapalace (add-one, add-$\alpha$) [\alert{unseen words same
  probability}] 
\item  Deleted (held-out) estimation [\alert{things-we-saw-once}]
\item  Good Turing [\alert{things new}, assuming leave-one-out]
\item  Back-off (Katz) smoothing [\alert{non-linear} zero-count]
\item  Interpolation (Jelinek-Mercer) smoothing [\alert{linear} zero-count]
\item  Witten-Bell [interpolation, \alert{diversity} of predicted word]
\item  Absolute discounting [\alert{approximating} Good-Turing]
\item  Kneser-Ney [back-off, \alert{diversity} of history]
\item  Modified Kneser-Ney [\alert{interpolated back-off}]
\end{enumerate}
\item  Managing the size of the model
\end{itemize}

\end{frame}
%------------------------------------------------

%------------------------------------------------
%------------------------------------------------
\begin{frame} \frametitle{References}
Many slides are from:
\begin{itemize}
\item The \alert{StatML} book's Web site \&
\item \alert{Dan Jurafsky}'s
``Language Modeling: Introduction to N-grams''. \&
\item \alert{Bill MacCartney}'s ``NLP Lunch Tutorial: Smoothing''
\end{itemize}
\begin{center}
\Huge\alert {Thank You!}
\end{center}

\end{frame}
%------------------------------------------------
%------------------------------------------------
\begin{frame}
\Huge{\centerline{The End}}
\end{frame}

%----------------------------------------------------------------------------------------

\end{document} 